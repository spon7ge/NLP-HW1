{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f8266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e39ec",
   "metadata": {},
   "source": [
    "#### 1. Evaluation Metrics ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f96d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input: y_pred, a list of length n with the predicted labels,\n",
    "## y_true, a list of length n with the true labels\n",
    "#                Pred\n",
    "#              |   0   |   1\n",
    "# true    0    |  TN   |  FP\n",
    "#         1    |  FN   |  TP\n",
    "\n",
    "## Calculates the precision of the predicted labels\n",
    "def get_precision(y_pred, y_true):\n",
    "    truePos, falsePos = 0, 0\n",
    "    for i,j in zip(y_pred, y_true):\n",
    "        if i == 1 and j == 1:\n",
    "            truePos += 1\n",
    "        elif i == 1 and j == 0:\n",
    "            falsePos += 1\n",
    "            \n",
    "    if truePos + falsePos == 0: #incase there are no true positives or false positives\n",
    "        return 0\n",
    "    \n",
    "    precision = truePos/(truePos + falsePos)\n",
    "    return precision\n",
    "    \n",
    "## Calculates the recall of the predicted labels\n",
    "def get_recall(y_pred, y_true): #TP/(TP + FN)\n",
    "    truePos, falseNeg = 0, 0\n",
    "    \n",
    "    for i,j in zip(y_pred, y_true):\n",
    "        if i == 1 and j == 1:\n",
    "            truePos += 1\n",
    "        elif i == 0 and j == 1:\n",
    "            falseNeg += 1\n",
    "    recall = truePos/(truePos+falseNeg)\n",
    "    return recall\n",
    "\n",
    "## Calculates the f-score of the predicted labels\n",
    "def get_fscore(y_pred, y_true): #TP/(TP + 1/2(FP + FN))\n",
    "    truePos, falsePos, falseNeg = 0, 0, 0\n",
    "    for i, j in zip(y_pred,y_true):\n",
    "        if i == 1 and j == 1:\n",
    "            truePos += 1\n",
    "        elif i == 1 and j == 0:\n",
    "            falsePos += 1\n",
    "        elif i == 0 and j == 1:\n",
    "            falseNeg += 1\n",
    "    if truePos == 0:\n",
    "        return 0\n",
    "    fscore = truePos/(truePos + 0.5 * (falsePos + falseNeg))\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf9ff6e",
   "metadata": {},
   "source": [
    "#### 2. Complex Word Identification ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c9b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(data_file):\n",
    "    words = []\n",
    "    labels = []   \n",
    "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words, labels\n",
    "\n",
    "## Makes feature matrix for all complex\n",
    "def all_complex_feature(words):\n",
    "    res = []\n",
    "    for word in words:\n",
    "        res.append([1])\n",
    "    return res\n",
    "\n",
    "## Labels every word complex\n",
    "def all_complex(data_file):\n",
    "    ## YOUR CODE HERE...\n",
    "    words, labels = load_file(data_file)\n",
    "    pred = [1] * len(words)\n",
    "\n",
    "    # metrics\n",
    "    precision = get_precision(pred, labels)\n",
    "    recall = get_recall(pred, labels)\n",
    "    fscore = get_fscore(pred, labels)\n",
    "\n",
    "    performance = [precision, recall, fscore]\n",
    "    return performance\n",
    "\n",
    "\n",
    "# ### 2.2: Word length thresholding\n",
    "\n",
    "## Makes feature matrix for word_length_threshold\n",
    "def length_threshold_feature(words, threshold):\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if len(word) >= threshold:\n",
    "            res.append([1])\n",
    "        else:\n",
    "            res.append([0])\n",
    "    return res\n",
    "\n",
    "## Finds the best length threshold by f-score, and uses this threshold to\n",
    "## classify the training and development set\n",
    "def word_length_threshold(training_file, development_file):\n",
    "    ## YOUR CODE HERE\n",
    "    words, labels = load_file(training_file)\n",
    "    best_fscore = 0\n",
    "    best_length = 4\n",
    "    \n",
    "    for i in range(4,11):\n",
    "        res = length_threshold_feature(words,i)\n",
    "        pred = [x[0] for x in res]\n",
    "        \n",
    "        tprecision = get_precision(pred,labels)\n",
    "        trecall = get_recall(pred,labels)\n",
    "        tfscore = get_fscore(pred,labels)\n",
    "        if tfscore > best_fscore:\n",
    "            best_fscore = tfscore\n",
    "            best_length = i\n",
    "            best_precision = tprecision\n",
    "            best_recall = trecall \n",
    "    \n",
    "    dev_words, dev_labels = load_file(development_file)\n",
    "    dev_res = length_threshold_feature(dev_words,best_length)\n",
    "    dev_pred = [x[0] for x in dev_res]\n",
    "    \n",
    "    dprecision = get_precision(dev_pred,dev_labels)\n",
    "    drecall = get_recall(dev_pred,dev_labels)\n",
    "    dfscore = get_fscore(dev_pred,dev_labels)\n",
    "            \n",
    "    training_performance = [best_precision, best_recall, best_fscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a0c2ec",
   "metadata": {},
   "source": [
    "### 2.3: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aa47755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains a Naive Bayes classifier using length and frequency features\n",
    "from syllables import count_syllables\n",
    "\n",
    "vowels = \"aeiou\"\n",
    "def getFeatures(word, counts):\n",
    "    wordFeatures = []\n",
    "    count = 0\n",
    "    wordFeatures.append(len(word)) #length of word\n",
    "    wordFeatures.append(count_syllables(word)) #number of syllables\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            count += 1\n",
    "    wordFeatures.append(count) #number of vowels in each word\n",
    "    wordFeatures.append(word.count('y')) \n",
    "    wordFeatures.append(len([c for c in word if c not in vowels])) \n",
    "    # wordFreq = counts.get(word,0)\n",
    "    # if wordFreq > 10:\n",
    "    #     wordFreq = 10 #capping at 10 for numbers with high frequency\n",
    "    # wordFeatures.append(wordFreq) #frequency of word\n",
    "    return wordFeatures\n",
    "\n",
    "counts = {}\n",
    "def simpleCounts(training_file):\n",
    "    words, labels = load_file(training_file)\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def naive_bayes(training_file, development_file, counts):\n",
    "    training_words, training_labels = load_file(training_file)\n",
    "    development_words, development_labels = load_file(development_file)\n",
    "    \n",
    "    # Extract features for training\n",
    "    tFeatures = []\n",
    "    for word in training_words:\n",
    "        tFeatures.append(getFeatures(word, counts))\n",
    "    \n",
    "    # Count feature occurrences\n",
    "    featureProb = {}\n",
    "    for i, features in enumerate(tFeatures):\n",
    "        label = training_labels[i]\n",
    "        for idx, value in enumerate(features):\n",
    "            if idx not in featureProb:\n",
    "                featureProb[idx] = {}\n",
    "            if value not in featureProb[idx]:\n",
    "                featureProb[idx][value] = {'complex': 0, 'simple': 0}\n",
    "            if label == 1: #count complex words\n",
    "                featureProb[idx][value]['complex'] += 1\n",
    "            else: #count simple words\n",
    "                featureProb[idx][value]['simple'] += 1\n",
    "    \n",
    "    # Calculate class probabilities\n",
    "    probComplex = sum(1 for label in training_labels if label == 1)\n",
    "    probSimple = sum(1 for label in training_labels if label == 0)\n",
    "    probComplex /= len(training_labels)\n",
    "    probSimple /= len(training_labels)\n",
    "\n",
    "    # Convert feature counts to probabilities\n",
    "    total_complex_words = sum(1 for label in training_labels if label == 1)\n",
    "    total_simple_words = sum(1 for label in training_labels if label == 0)\n",
    "\n",
    "    for feature_idx in featureProb:\n",
    "        for feature_value in featureProb[feature_idx]:\n",
    "            featureProb[feature_idx][feature_value]['complex'] /= total_complex_words\n",
    "            featureProb[feature_idx][feature_value]['simple'] /= total_simple_words\n",
    "\n",
    "    # Predict development set\n",
    "    devPred = []\n",
    "    for word in development_words:\n",
    "        wordFeatures = getFeatures(word, counts)\n",
    "        scoreComplex = probComplex\n",
    "        scoreSimple = probSimple\n",
    "        \n",
    "        for feature_idx, featureValue in enumerate(wordFeatures):\n",
    "            # Look up probabilities, with default values if feature wasn't seen in training\n",
    "            if feature_idx in featureProb and featureValue in featureProb[feature_idx]:\n",
    "                prob_complex = featureProb[feature_idx][featureValue]['complex']\n",
    "                prob_simple = featureProb[feature_idx][featureValue]['simple']\n",
    "            else:\n",
    "                # Handle unseen feature values with small default probabilities\n",
    "                prob_complex = 1e-6  # Very small probability\n",
    "                prob_simple = 1e-6\n",
    "            \n",
    "            # Multiply probabilities (Naive Bayes assumption)\n",
    "            scoreComplex *= prob_complex\n",
    "            scoreSimple *= prob_simple\n",
    "            \n",
    "        if scoreComplex > scoreSimple:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "        devPred.append(prediction)\n",
    "    \n",
    "    # Predict training set\n",
    "    trainingPred = []\n",
    "    for word in training_words:\n",
    "        wordFeatures = getFeatures(word, counts)\n",
    "        scoreComplex = probComplex\n",
    "        scoreSimple = probSimple\n",
    "        \n",
    "        for feature_idx, featureValue in enumerate(wordFeatures):\n",
    "            if feature_idx in featureProb and featureValue in featureProb[feature_idx]:\n",
    "                prob_complex = featureProb[feature_idx][featureValue]['complex']\n",
    "                prob_simple = featureProb[feature_idx][featureValue]['simple']\n",
    "            else:\n",
    "                prob_complex = 1e-6\n",
    "                prob_simple = 1e-6\n",
    "            scoreComplex *= prob_complex\n",
    "            scoreSimple *= prob_simple\n",
    "        \n",
    "        if scoreComplex > scoreSimple:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = 0\n",
    "        trainingPred.append(prediction)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tprecision = get_precision(trainingPred, training_labels)\n",
    "    trecall = get_recall(trainingPred, training_labels)\n",
    "    tfscore = get_fscore(trainingPred, training_labels)\n",
    "    \n",
    "    dprecision = get_precision(devPred, development_labels)\n",
    "    drecall = get_recall(devPred, development_labels)\n",
    "    dfscore = get_fscore(devPred, development_labels)\n",
    "    \n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33505a23",
   "metadata": {},
   "source": [
    "I removed word frequency due to the high numbers i was getting. It was also causing my logistic regression model to give me a recall score of 99%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Development - Precision: 0.685\n",
      "Development - Recall: 0.730\n",
      "Development - F-score: 0.707\n"
     ]
    }
   ],
   "source": [
    "train = 'complex_words_training.txt'\n",
    "dev = 'complex_words_development.txt'\n",
    "test = 'complex_words_test_unlabeled.txt'\n",
    "\n",
    "counts = simpleCounts(train)\n",
    "results = naive_bayes(train, dev, counts)\n",
    "\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Precision: {results[0]:.3f}\")\n",
    "print(f\"Recall: {results[1]:.3f}\")\n",
    "print(f\"F-score: {results[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba52c36",
   "metadata": {},
   "source": [
    "### 2.4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2826d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains a Logistic Regression classifier using length and frequency features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "def logistic_regression(training_file, development_file, counts):\n",
    "    training_words, training_labels = load_file(training_file)\n",
    "    development_words, development_labels = load_file(development_file)\n",
    "    \n",
    "    # Extract features for training\n",
    "    training_features = []\n",
    "    for word in training_words:\n",
    "        training_features.append(getFeatures(word, counts))\n",
    "    \n",
    "    # Extract features for development\n",
    "    development_features = []\n",
    "    for word in development_words:\n",
    "        development_features.append(getFeatures(word, counts))\n",
    "    \n",
    "    # Convert to numpy arrays (required by sklearn)\n",
    "    X_train = np.array(training_features)\n",
    "    y_train = np.array(training_labels)\n",
    "    X_dev = np.array(development_features)\n",
    "    y_dev = np.array(development_labels)\n",
    "    \n",
    "    # Create and train logistic regression model\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    training_predictions = model.predict(X_train)\n",
    "    development_predictions = model.predict(X_dev)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    tprecision = get_precision(training_predictions.tolist(), training_labels)\n",
    "    trecall = get_recall(training_predictions.tolist(), training_labels)\n",
    "    tfscore = get_fscore(training_predictions.tolist(), training_labels)\n",
    "    \n",
    "    dprecision = get_precision(development_predictions.tolist(), development_labels)\n",
    "    drecall = get_recall(development_predictions.tolist(), development_labels)\n",
    "    dfscore = get_fscore(development_predictions.tolist(), development_labels)\n",
    "    \n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9142e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Development - Precision: 0.725\n",
      "Development - Recall: 0.629\n",
      "Development - F-score: 0.673\n"
     ]
    }
   ],
   "source": [
    "logisticResults = logistic_regression(train, dev, counts)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Precision: {logisticResults[0]:.3f}\")\n",
    "print(f\"Recall: {logisticResults[1]:.3f}\")\n",
    "print(f\"F-score: {logisticResults[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3c348",
   "metadata": {},
   "source": [
    "### 2.7: Build your own classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06178e8",
   "metadata": {},
   "source": [
    "I chose to use random forest classifer for the model of my choice. \n",
    "How it works, you create many trees but each tree sees different data, and each tree uses random features to come with the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d12b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains a Random Forest classifier using length and syllable features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def random_forest(training_file, development_file, counts):\n",
    "    training_words, training_labels = load_file(training_file)\n",
    "    development_words, development_labels = load_file(development_file)\n",
    "    \n",
    "    training_features = [] #grabs the features for the training set\n",
    "    for word in training_words:\n",
    "        training_features.append(getFeatures(word, counts))\n",
    "    \n",
    "    development_features = [] #grabs the features for the development set\n",
    "    for word in development_words:\n",
    "        development_features.append(getFeatures(word, counts))\n",
    "    \n",
    "    X_train = np.array(training_features)\n",
    "    y_train = np.array(training_labels)\n",
    "    X_dev = np.array(development_features)\n",
    "    y_dev = np.array(development_labels)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,      # Number of trees\n",
    "        random_state=42,       \n",
    "        max_depth=10,          # try to prevent overfitting\n",
    "        min_samples_split=5,   # Minimum samples to split a node\n",
    "        min_samples_leaf=2     # Minimum samples in a leaf\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    training_predictions = model.predict(X_train)\n",
    "    development_predictions = model.predict(X_dev)\n",
    "    \n",
    "    #metrics\n",
    "    tprecision = get_precision(training_predictions.tolist(), training_labels)\n",
    "    trecall = get_recall(training_predictions.tolist(), training_labels)\n",
    "    tfscore = get_fscore(training_predictions.tolist(), training_labels)\n",
    "    \n",
    "    dprecision = get_precision(development_predictions.tolist(), development_labels)\n",
    "    drecall = get_recall(development_predictions.tolist(), development_labels)\n",
    "    dfscore = get_fscore(development_predictions.tolist(), development_labels)\n",
    "    \n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb5813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Development - Precision: 0.702\n",
      "Development - Recall: 0.648\n",
      "Development - F-score: 0.674\n"
     ]
    }
   ],
   "source": [
    "rf_performance = random_forest(train, dev, counts)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Development - Precision: {rf_performance[0]:.3f}\")\n",
    "print(f\"Development - Recall: {rf_performance[1]:.3f}\")\n",
    "print(f\"Development - F-score: {rf_performance[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92c34d",
   "metadata": {},
   "source": [
    "BEST PRECISION: LOGISTIC REGRESSION WITH *0.725*\n",
    "\n",
    "BEST RECALL: NAIVE BAYES WITH *0.730* \n",
    "\n",
    "BEST F-SCORE: NAIVE BAYES WITH *0.707*\n",
    "\n",
    "Naive bayes showed to be the best model out of the 3 which uses probability-based classification. \n",
    "\n",
    "Features used:\n",
    "- Word length\n",
    "- Syllable Count\n",
    "- Vowel Count\n",
    "- Word Frequency(i removed it since it wasnt helping my models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
